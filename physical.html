<!DOCTYPE html>
<html>
	<head>
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">

		<!-- Bootstrap requires jQuery -->
		<script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>

		<!-- Load some Lora -->
		<link href="https://fonts.googleapis.com/css2?family=Lora:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">

		<!-- Latest compiled and minified CSS -->
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

		<!-- Optional theme -->
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">

		<!-- Latest compiled and minified JavaScript -->
		<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

		<link rel="stylesheet" href="style.css" />

		<title>Physical output</title>

	</head>
	<body>

		<p><a href="index.html">Back to table of contents</a></p>

		<img src="images/makerbot.jpg" class="img-responsive" alt="A Makerbot 3D printer printing a nested dodecahedron." />
		<small>A Makerbot 3D printer.</small>

		<h1>Physical Output</h1>

		<div class="lead">Amy J. Ko</div>

		<p>
			Whereas the three-dimensional output we discussed in <a href="3D.html">the previous chapter</a> can create entirely new virtual worlds, or enhanced versions of our own world, it's equally important that software be able to interface with the physical world.
			One of the first research papers to recognize this was the seminal work <em>Tangible Bits: Towards Seamless Interfaces between People, Bits and Atoms</em> (<a href="https://doi.org/10.1145/258549.258715">Ishii and Ullmer 2006</a>).
			In it, Ishii and Ullmer described a rift between the digital and physical world:
		</p>

		<blockquote>
			We live between two realms: our physical environment and cyberspace. Despite our dual citizenship, the absence of seamless couplings between these parallel existences leaves a great divide between the worlds of bits and atoms. At the present, we are torn between these parallel but disjoint spaces... Streams of bits leak out of cyberspace through a myriad of rectangular screens into the physical world as photon beams. However, the interactions between people and cyberspace are now largely confined to traditional GUI (Graphical User Interface)-based boxes sitting on desktops or laptops. The interactions with these GUIs are separated from the ordinary physical environment within which we live and interact. Although we have developed various skills and work practices for processing information through haptic interactions with physical objects (e.g., scribbling messages on Post-It&trade; notes and spatially manipulating them on a wall) as well as peripheral senses (e.g., being aware of a change in weather through ambient light), most of these practices are neglected in current HCI design because of the lack of diversity of input/output media, and too much bias towards graphical output at the expense of input from the real world.
		</blockquote>

		<p>
			This basic idea, that the massive rift between our physical and digital worlds should be bridged by a diversity of new physical input and output media, dovetailed Weiser's vision of ubiquitous computing (<a href="http://www.jstor.org/stable/24938718">Weiser 1991</a>).
			However, rather than envisioning a world of embedded computing, it focused on our embodied experiences with physical objects.
			Both of these themes are another example of how interfaces play a central role in <a href="mediation.html">mediating our interactions</a>.
		</p>

		<p>
			The impact of this vision was an explosion of research to create more physical input and output.
			In this chapter, we'll discuss this research, and the history of physical computing that it build upon.
		</p>

		<h2>Printing</h2>

		<p>
			Most people who've interacted with computers have used a printer at some point in their life to turn bits into atoms.
			The basic principle is simple: take a digital document and create a facsimile with paper and some kind of mark, such as spraying ink (inkjets), burning the paper (laser printers), or one of a variety of other approaches.
		</p>

		<p>
			Why was printing so important as computers first became ubiquitous?
			For most of the 20th century, paper was the central medium for transmitting information.
			We used typewriters to create documents.
			We stored documents in file cabinets.
			We used photocopiers to duplicate documents.
			The very notion of a file and folder in graphical user interfaces mirrored the ubiquity of interacting with paper.
			Printers were a necessary interface between the nascent digital world and the dominant paper-based world of information.
		</p>
		
		<p>
			One of the earliest forms of digital printing was the stock ticker machine, which printed text on a thin strip of paper on which messages are recorded:
		</p>

		<p>
			<center>
				<img src="images/tickertape.jpg" class="img-responsive" alt="Two women reading ticker tape at the New York Stock Exchange." />
				<small>Two women at the Waldorf-Astoria Hotel in 1918 operating the ticker machines and stock exchange boards. Credit: U.S. War Department.</small>
			</center>
		</p>

		<p>
			While ticker tape was the most ubiquitous before the advent of digital computers, printing devices had long been in the imagination of early computer scientists.
			<a href="https://de.wikipedia.org/wiki/Charles_Babbage">Charles Babbage</a> was an English mathematician and philosopher who first imagined the concept of a programmable digital computer in 1822.
			He also imagined, however, a mechanical printing device that could print the results of his imagined differencing machine.
			Eventually, people began to engineer these printing devices.
			For example, consider the dot matrix printer shown in the video below, which printed a grid of ink.
			These printers were ubiquitous in the 1980's, and a general extension of both ticker tape printers and a deeper integration with digital printing from general purpose computers.
			This was the beginning of a much greater diversity of printing mediums we use today, which use toner, liquid ink, solid ink, or dye-sublimation&mdash;.
		</p>

		<p class="embed-responsive embed-responsive-16by9">
			<iframe width="560" height="315" src="https://www.youtube.com/embed/u8I6qt_Z0Cg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
			<center>Dot matrix printers can also be used to play inspiring montage music.</center>
		</p>

		<p>
			Most of these printing technologies have focused on 2-dimensional output because the documents we create on computers are primarily 2-dimensional.
			However, as the plastics industry evolved, and plastic extruders reshaped manufacturing, interest in democratizing access to 3D fabrication expanded. This led to the first 3D printer, described in a U.S. patent in 1984, which described a process for generating 3D objects by creating cross-sectional patterns of an object (<a href="https://www.google.com/patents/US4575330">Hull 1984</a>).
			These early visions for personal 3D printing, combined with decades of research on the basic technologies required to manufacture printers at scale, eventually led to a new market for 3D printing, including companies like <a href="https://www.makerbot.com/">MakerBot</a>.
			These printers also required the creation of new software to model printable 3D forms.
		</p>

		<p>
			While the basic idea of 3D printing is now well established, and the market for 3D printers is expanding, researchers have gone well beyond the original premise.
			Much of this exploration has been in exploring materials other than plastic.
			One example of this is an approach to printing <em>interactive electromechanical objects with wound in place coils</em> (<a href="https://doi.org/10.1145/2984511.2984523">Peng et al. 2016</a>).
			These objects, made of copper wire, and structural plastic elements, allows for the printing of objects like actuated arms of toys, electric motors, and electronic displays:
		</p>

		<p class="embed-responsive embed-responsive-16by9">
	        <iframe width="560" height="315" src="https://www.youtube.com/embed/5MEYe-z9GKo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
			<center>Printing electromagnetic objects.</center>
		</p>

		<p>
			Another project explored printing with wool and wool blend yarn to create soft felt objects rather than just rigid plastic objects (<a href="https://doi.org/10.1145/2556288.2557338">Hudson 2014</a>).
			Techniques like this and the one above simultaneously innovate in printing techniques, but also in exploring the possibilities of new media for bridging the digital and physical world.
		</p>

		<p class="embed-responsive embed-responsive-16by9">
			<iframe width="560" height="315" src="https://www.youtube.com/embed/YhXIWyfI7Cw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
			<center>Printing with felt.</center>
		</p>

		<p>
			These new forms of printing pose new challenges in authoring.
			One cannot print arbitrary 3D shapes with 3D printers, and so this requires authors to understand the limitations of printing methods.
			This lack of understanding can lead to failed prints, which can be frustrating and expensive.
			Some researchers have explored "patching" 3D objects, calculating additional parts that must be printed and mounted onto an existing failed print to create the desired shape (<a href="https://doi.org/10.1145/2807442.2807467">Teubrich et al. 2015</a>).
			Other research has investigated new software tools for extending, repairing, or modifying everyday objects by modeling those objects and streamlining the creation of attachment objects (<a href="https://doi.org/10.1145/2807442.2807498">Chen et al. 2015</a>):
		</p>

		<p class="embed-responsive embed-responsive-16by9">
	        <iframe width="560" height="315" src="https://www.youtube.com/embed/obui6I2dzxk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
			<center>Designing and printing attachment objects.</center>
		</p>

		<p>These techniques, while not advancing <em>how</em> objects are printed, innovate in how we author objects to print, much like research innovations in word processing software from the 1980's and 1990's.</p>

		<h2>Morphing</h2>

		<p>
			Whereas printing is all about creating physical form to digital things, another critical bridge between the physical and digital world is adapting digital things to our physical world.
			Screens, for example, are one of the key ways that we present digital things, but their rigidity has a way of defining the physical form of objects, rather than objects defining the physical form of screens.
			The iPhone is a flat sheet of glass, defined by the screen; a laptop is shape of a screen.
			Researchers have long pondered why are screens have to be flat, and have envisioned different forms of output that might have new unimagined benefits.
		</p>
	
		<p>		
			Some research has focused on making screens flexible and bendable.
			For example, one technique takes paper, plastics, and fabrics, and makes it easy to create programmable shape-changing behaviors with those materials (<a href="https://doi.org/10.1145/2984511.2984520">Ou et al. 2016</a>).
			As seen in the video below, this creates a new kind of digital display that can dynamically convey information and shape.
		</p>

		<p class="embed-responsive embed-responsive-16by9">
        	<iframe width="560" height="315" src="https://www.youtube.com/embed/b1vrvDUr1Ds" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
			<center>Programmable, shape-changing materials.</center>
		</p>

		<p>
			Other projects have created <em>stretchable</em> user interfaces with sensing capabilities and visual output, allowing for conventional experiences in unconventional places (<a href="https://doi.org/10.1145/2984511.2984521">Wessley et al. 2016</a>).
			As seen in the video below, these screens adapt to objects' physical forms, rather than requiring objects to adapt to a rectangular display:
		</p>

		<p class="embed-responsive embed-responsive-16by9">
      	  <iframe width="560" height="315" src="https://www.youtube.com/embed/gPVdvxfeE50" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
			<center>Stretchable interfaces.</center>
		</p>

		<p>
			Some research has even explored foldable interactive objects by using thin-film printed electronics (<a href="https://doi.org/10.1145/2807442.2807494">Olberding et al. 2015</a>).
			The video below shows several examples of the types of new interactive physical forms that such materials might enable:
		</p>

		<p class="embed-responsive embed-responsive-16by9">
	        <iframe width="560" height="315" src="https://www.youtube.com/embed/yQjhe0Ravf0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
			<center>Foldable interfaces.</center>
		</p>

		<p>
			This line of research innovates in both the industrial forms that interfaces take, while offering new possibilities for how they are manipulated physically.
			It envisions a world in which digital information might be presented and interacted with in the shape most suitable to the data, rather than adapting the data to the shape of a screen.
			Most of these innovation efforts are not driven by problems with existing interfaces, but opportunities for new experiences that we have not yet envisioned.
			With many innovations from research such as foldable displays now make it to market, we can see how technology-driven innovation, versus problem-driven innovation, can struggle to demonstrate value in the marketplace.
		</p>

		<h2>Haptics</h2>
        
        <p>
	        Whereas morphing interfaces change the <em>structural</em> properties of the interface forms, others have focused on offering physical, tangible feedback.
	        Feedback is also critical to achieving the vision of tangible bits, as the more physical our devices become, the more they need to communicate back to us through physical rather than visual form.
	        We call physical feedback <strong>haptic</strong> feedback, because it leverages people's perception of touch and sense of where their body is in space (known as proprioception).
        </p>

        <p>
	        Haptic feedback varies in its goals.
	        For instance, some haptic feedback operates at a low level of human performance, such as this idea, which recreates the physical sensation of writing on paper with a pencil, ballpoint pen or marker pen, but with a stylus (<a href="https://doi.org/10.1145/2984511.2984550">Cho et al. 2016</a>):
	       </p>

		<p class="embed-responsive embed-responsive-16by9">
	        <iframe width="560" height="315" src="https://www.youtube.com/embed/RNn0_rkFXkw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
			<center>Simulated writing feedback.</center>
		</p>

		<p>
			Other haptic feedback aims to provide feedback about user interface behavior using physical force.
			For example, one project used electrical muscle stimulation to steer the user's wrist while plotting charts, filling in forms, and other tasks, to prevent errors (<a href="https://doi.org/10.1145/2984511.2984530">Lopes et al. 2016</a>).
			Another used airflow to provide feedback without contact (<a href="https://doi.org/10.1145/2984511.2984583">Lee and Lee 2016</a>):
		</p>

		<p class="embed-responsive embed-responsive-16by9">
        	<iframe width="560" height="315" src="https://www.youtube.com/embed/QdSK_K3Spcw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
			<center>Feedback with airflow.</center>
		</p>

		<p>
			Many projects have used haptics to communicate detailed shape information, helping to bring tactile information to visual virtual worlds.
			Some have used ultrasound to project specific points of feedback onto hands in midair (<a href="https://doi.org/10.1145/2501988.2502018">Carter et al. 2013</a>).
			Others used electrovibration, integrated with touch input, enabling the design interfaces that allow users to feel virtual elements (<a href="https://doi.org/10.1145/1866029.1866074">Bau et al. 2010</a>).
			Some projects have used physical forms to provide shape information, such as this gel-based surface imposed on a multi-touch sensor allowing for freely morphed arbitrary shapes (<a href="https://doi.org/10.1145/2807442.2807487">Miruchna et al. 2015</a>):
		</p>

		<p class="embed-responsive embed-responsive-16by9">
	        <iframe width="560" height="315" src="https://www.youtube.com/embed/o8W6qbwPhwU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
			<center>Gel-based multi-touch tactile feedback.</center>
		</p>

		<p>
			Some work leverages <strong>visuo-haptic illusions</strong> to successfully trick a user's mind into feeling something virtual.
			For example, one work displayed a high resolution visual form with tactile feedback from a low-resolution grid of actuated pins that move up and down, giving a sense of high-resolution tactile feedback (<a href="https://doi.org/10.1145/3173574.3173724">Abtahi et al 2018</a>).
			Some projects have created a sense of virtual motion by varying waves of actuator motion left or right (<a href="https://doi.org/10.1145/2380296.2380300">Kaye 2012</a>).
			In anticipation of virtual reality simulations of combat, some have even created perceptions of being physically hit by tapping the skin gently while thrusting a user's arm backwards using electrical muscle stimulation (<a href="https://doi.org/10.1145/2807442.2807443">Lopes et al. 2015</a>):
		</p>

		<p class="embed-responsive embed-responsive-16by9">
	        <iframe width="560" height="315" src="https://www.youtube.com/embed/CNEZguz1NEU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
			<center>Simulating impact.</center>
		</p>

		<p>
			All of these approaches to haptic feedback bridge the digital and physical worlds by letting information from digital world to reach our tactile senses.
			In a sense, all haptic feedback is about bridging gulfs of evaluation in physical computing: in a physical device, how can the device communicate that it's received input and clearly convey its response?
		</p>

		<hr/>

		<p>
			While this exploration of media for bridging bits and atoms has been quite broad, it is not yet deep.
			Many of these techniques are only just barely feasible, and we still know little about what we might do with these techniques, how useful or valued these applications might be, or what it would take to manufacture and maintain the hardware they require.
			Nevertheless, it's clear that the tangible bits that Ishii and Ullmer envisioned are not only possible, but rich, under-explored, and potentially transformative.
			As the marketplace begins to build some of these innovations into products, we will begin to see just how valuable these innovations are in practice.
		</p>

		<center><p class="lead"><a href="help.html">Next chapter: Help</a></p></center>

		<h2>Further reading</h2>

		<p>Parastoo Abtahi, and Sean Follmer. 2018. <a href="http://hci.stanford.edu/publications/2018/haptic_illusions/Haptic_Illusions_paper.pdf">Visuo-haptic Illusions for Improving the Perceived Performance of Shape Displays</a>. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI '18). ACM, New York, NY, USA.</p>

		<p>Olivier Bau, Ivan Poupyrev, Ali Israr, and Chris Harrison. 2010. <a href="https://doi.org/10.1145/1866029.1866074">TeslaTouch: electrovibration for touch surfaces</a>. In Proceedings of the 23nd annual ACM symposium on User interface software and technology (UIST '10). ACM, New York, NY, USA, 283-292.</p>

		<p>Tom Carter, Sue Ann Seah, Benjamin Long, Bruce Drinkwater, and Sriram Subramanian. 2013. <a href="https://doi.org/10.1145/2501988.2502018">UltraHaptics: multi-point mid-air haptic feedback for touch surfaces</a>. In Proceedings of the 26th annual ACM symposium on User interface software and technology (UIST '13). ACM, New York, NY, USA, 505-514.</p>

		<p>Xiang 'Anthony' Chen, Stelian Coros, Jennifer Mankoff, and Scott E. Hudson. 2015. <a href="https://doi.org/10.1145/2807442.2807498">Encore: 3D Printed Augmentation of Everyday Objects with Printed-Over, Affixed and Interlocked Attachments</a>. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology (UIST '15). ACM, New York, NY, USA, 73-82.</p>

		<p>Artem Dementyev, Hsin-Liu (Cindy) Kao, Inrak Choi, Deborah Ajilo, Maggie Xu, Joseph A. Paradiso, Chris Schmandt, and Sean Follmer. 2016. <a href="https://doi.org/10.1145/2984511.2984531">Rovables: Miniature On-Body Robots as Mobile Wearables</a>. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST '16). ACM, New York, NY, USA, 111-120.</p>

		<p>Scott E. Hudson. 2014. <a href="https://doi.org/10.1145/2556288.2557338">Printing teddy bears: a technique for 3D printing of soft interactive objects</a>. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '14). ACM, New York, NY, USA, 459-468.</p>

		<p>Hiroshi Ishii and Brygg Ullmer. (1997). <a href="https://doi.org/10.1145/258549.258715">Tangible bits: Towards seamless interfaces between people, bit, and atoms</a>. Proceedings of the ACM Conference on Human Factors in Computing Systems (CHI ’97). Atlanta, Georgia (March 22-27, 1997). New York: ACM Press, pp. 234-241.</p>

		<p>Jaeyeon Lee and Geehyuk Lee. 2016. <a href="https://doi.org/10.1145/2984511.2984583">Designing a Non-contact Wearable Tactile Display Using Airflows</a>. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST '16). ACM, New York, NY, USA, 183-194.</p>

		<p>Mathieu Le Goc, Lawrence H. Kim, Ali Parsaei, Jean-Daniel Fekete, Pierre Dragicevic, and Sean Follmer. 2016. <a href="https://doi.org/10.1145/2984511.2984547">Zooids: Building Blocks for Swarm User Interfaces</a>. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST '16). ACM, New York, NY, USA, 97-109.</p>

		<p>Joseph 'Jofish' Kaye. 2012. <a href="https://doi.org/10.1145/2380296.2380300">Sawtooth planar waves for haptic feedback</a>. In Adjunct proceedings of the 25th annual ACM symposium on User interface software and technology (UIST Adjunct Proceedings '12). ACM, New York, NY, USA, 5-6.</p>

		<p>Pedro Lopes, Alexandra Ion, and Patrick Baudisch. 2015. <a href="https://doi.org/10.1145/2807442.2807443">Impacto: Simulating Physical Impact by Combining Tactile Stimulation with Electrical Muscle Stimulation</a>. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology (UIST '15). ACM, New York, NY, USA, 11-19.</p>

		<p>Pedro Lopes, Doăa Yüksel, François Guimbretière, and Patrick Baudisch. 2016. <a href="https://doi.org/10.1145/2984511.2984530">Muscle-plotter: An Interactive System based on Electrical Muscle Stimulation that Produces Spatial Output</a>. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST '16). ACM, New York, NY, USA, 207-217.</p>

		<p>Viktor Miruchna, Robert Walter, David Lindlbauer, Maren Lehmann, Regine von Klitzing, and Jörg Müller. 2015. <a href="https://doi.org/10.1145/2807442.2807487">GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel</a>. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology (UIST '15). ACM, New York, NY, USA, 3-10.</p>

		<p>Ken Nakagaki, Artem Dementyev, Sean Follmer, Joseph A. Paradiso, and Hiroshi Ishii. 2016. <a href="https://doi.org/10.1145/2984511.2984587">ChainFORM: A Linear Integrated Modular Hardware System for Shape Changing Interfaces. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST '16). ACM, New York, NY, USA, 87-96.</p>

		<p>Huaishu Peng, François Guimbretière, James McCann, and Scott Hudson. 2016. <a href="https://doi.org/10.1145/2984511.2984523">A 3D Printer for Interactive Electromagnetic Devices</a>. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST '16). ACM, New York, NY, USA, 553-562.</p>

		<p>Oliver S. Schneider, Ali Israr, and Karon E. MacLean. 2015. <a href="https://doi.org/10.1145/2807442.2807470">Tactile Animation by Direct Manipulation of Grid Displays</a>. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology (UIST '15). ACM, New York, NY, USA, 21-30.</p>

		<p>Alexander Teibrich, Stefanie Mueller, François Guimbretière, Robert Kovacs, Stefan Neubert, and Patrick Baudisch. 2015. <a href="https://doi.org/10.1145/2807442.2807467">Patching Physical Objects</a>. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology (UIST '15). ACM, New York, NY, USA, 83-91.</p>

		<p>Simon Olberding, Sergio Soto Ortega, Klaus Hildebrandt, and Jürgen Steimle. 2015. <a href="https://doi.org/10.1145/2807442.2807494">Foldio: Digital Fabrication of Interactive and Shape-Changing Objects With Foldable Printed Electronics</a>. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology (UIST '15). ACM, New York, NY, USA, 223-232.</p>

		<p>Jifei Ou, Mélina Skouras, Nikolaos Vlavianos, Felix Heibeck, Chin-Yi Cheng, Jannik Peters, and Hiroshi Ishii. 2016. <a href="https://doi.org/10.1145/2984511.2984520">aeroMorph - Heat-sealing Inflatable Shape-change Materials for Interaction Design</a>. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST '16). ACM, New York, NY, USA, 121-132.</p>

		<p>Michael Wessely, Theophanis Tsandilas, and Wendy E. Mackay. 2016. <a href="https://doi.org/10.1145/2984511.2984521">Stretchis: Fabricating Highly Stretchable User Interfaces</a>. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST '16). ACM, New York, NY, USA, 697-704.</p>

		<script type="text/javascript">
		
			var _gaq = _gaq || [];
			_gaq.push(['_setAccount', 'UA-10917999-1']);
			_gaq.push(['_trackPageview']);
			
			(function() {
			var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
			ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
			var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
			})();
		
		</script>

	</body>

</html>
