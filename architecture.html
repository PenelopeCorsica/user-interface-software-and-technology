<!DOCTYPE html>
<html>
	<head>
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">

		<!-- Bootstrap requires jQuery -->
		<script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>

		<!-- Load some Lora -->
		<link href="https://fonts.googleapis.com/css2?family=Lora:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">

		<!-- Latest compiled and minified CSS -->
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
		
		<!-- Optional theme -->
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">
		
		<!-- Latest compiled and minified JavaScript -->
		<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
		
		<link rel="stylesheet" href="style.css" />
		
		<title>User interface software architecture</title>
		
	</head>
	<body>

		<p><a href="index.html">Back to table of contents</a></p>

		<img src="images/angola.jpg" class="img-responsive" alt="A modern architecture of an apartment building, figuratively resembling software architecture." />
		<small>Modern architecture in Luanda, Angola, as a metaphor for software architecture (Credit: Costa Lopes)</small>
		
		<h1>User interface software architecture</h1>
		<div class="lead">Amy J. Ko</div>

		<p>
			While the previous chapter discussed many of the seminal interaction paradigms we have invented for interacting with computers, we've discussed little about how all of the widgets, interaction paradigms, and other user interface ideas are actually <em>implemented</em> as software.
			This knowledge is obviously important for developers who implement buttons, scroll bars, gestures, and so on, but is this knowledge important for anyone else?
		</p>
		
		<p>
			I argue yes.
			First, a precise understanding of user interface implementation allows designers to have a precise understanding of user interface behavior.
			This helps designers and engineers who think of user interface components as the building blocks of a user interface to analyze limitations of interfaces, predict edge cases in their behavior, and discuss their behavior precisely.
			Knowing, for example, that a button only invokes its command <em>after</em> the mouse button is released allows one to reason about the assumptions a button makes about <em>ability</em>.
			The ability to hold a mouse button down, for example, isn't something that all people have, whether due to limited finger strength, motor tremors that lead to accidental button releases, or other motor-physical limitations.
			A user interface designer or engineer knowing these low-level implementation details is like violinist knowing whether a bow's strings are made from synthetic materials or Siberian horse tail.
			These details allow you to fully control what you make and how it behaves.
		</p>
		
		<p>
			Knowledge of user interface implementation might also be important if you want to invent <em>new</em> interface paradigms.
			A low-granularity of user interface implementation knowledge allows you to see exactly how current interfaces are limited, and empowers you to envision new interfaces that don't have those limitations.
			For example, when <a href="https://techcrunch.com/2017/06/07/that-new-keyboard-is-the-key-to-apples-macbook-update/">Apple redesigned their keyboards to have shallower depth</a>, their design team needed deeper knowledge than just "pushing a key sends a key code to the operating system."
			They needed to know the physical mechanisms that afford depressing a key, the tactile feedback those mechanisms provide, and the auditory feedback that users rely on to confirm they've pressed a key.
			Expertise in these physical qualities of the hardware interface of a keyboard was essential to designing a new keyboard experience.
		</p>
		
		<p>
			Precise technical knowledge of user interface implementation also allows designers and engineers to have a shared vocabulary to <em>communicate</em> about interfaces.
			Designers should feel empowered to converse about interface implementation with engineers, knowing enough to critique designs and convey alternatives.
			Without this vocabulary and a grasp of these concepts, engineers retain power over user interface design, even though they aren't trained to design interfaces.
		</p>

		<h2>Architecture</h2>
		
		<p>
			What is this knowledge of implementation?
			I'm not proposing that everyone memorize and understand, for example, the source code implementations of all of Windows or macOS's widgets.
			I'm talking about a level of knowledge at the <strong>architectural</strong> level, which involves understandings patterns of control and data flow that govern user interface behavior.
		</p>
		
		<p>
			To illustrate this notion of architecture, let's return to the example of a graphical user interface button.
			We've all used buttons, but rarely do we think about exactly how they work.
			Here is the <em>architecture</em> of a very simple button:
		</p>

		<img src="images/button-architecture.jpg" class="img-responsive" alt="A diagram of two modes of a simple button: up and down, with a state transition on mouse down and a state transition on mouse up."/>

		<p>
			This is a simple <strong>state machine</strong> with two states: <em>mouse up</em> (on the left) and <em>mouse down</em> (on the right).
			<em>Events</em> cause the machine to transition between states: when the button receives a <em>mouse down</em> event, it transitions to the down state. 
			When it receives a <em>mouse up</em> event, it transitions to the up state and also executes its command. 
			This is about as simple as buttons get.
		</p>
		
		<p>
			Of course, graphical user interface buttons in modern operating systems are actually much more complex.
			Consider, for example, what happens if the button is in a down state, but the mouse cursor moves outside the boundary of the button and <em>then</em> a mouse up event occurs.
			When this happens, it transitions to the mouse up state, but does <em>not</em> execute the button's command.
			(Try this on a touchscreen: tap on a button, slide your finger away from it, then release, and you'll see the button's command isn't executed).
			If the button can be disabled, then there's an entirely different state of disabled in which the button never changes states in response to events.
			If the button supports touch input, then a button's state machine also needs to handle touch events.
			All of these additional behaviors add more complexity to a button's state machine, but ultimately, it is still state machine.
		</p>
		
		<p>
			All user interface widgets are implemented in similar ways, defining a set of states and events that cause transitions between those states.
			Scroll bar handles respond to mouse down, drag, and mouse up events, text boxes respond to keyboard events, links respond to mouse down and up events.
			Even text in a web browser or text editor response to mouse down, drag, and mouse up events to enter a "text selected state".
		</p>
		
		<p>
			A nearly ubiquitous way to implement these state machines is to follow a <strong>model-view-controller</strong> (MVC) architecture.
			One can think of MVC as a division of responsibilities between storing data, showing data, and manging the interaction between the storage and the display.
			For example, think about the signs that are often displayed on gas station or movie theaters.
			Someone is responsible for writing and storing the content that will be shown on the signs.
			Someone else is responsible for putting the content on the signs.
			And someone is in charge of managing the communication between the two, such as a manager, who listens  decides when the sign will change.
			In the same way, MVC architectures in user interfaces take an individual component of an interface (e.g., a button, a form, a progress bar, or some other widget) and divide its implementation into three parts:
		</p>

		<img src="images/mvc.jpg" class="img-responsive" alt="An MVC architecture for a web form, showing a model that stores the username and password being entered, the controller that decides when to activate the login button, and the view, which displays the form data and buttons, and listens for clicks."/>

		<ul>
			<li>
				The <strong>model</strong> stores the <em>data</em> that a user interface is presenting.
				For example, in the figure above, the model stores the username and password a user is currently entering.
				(In many user interface toolits, this wouldn't be stored in a separate database, but in the text field object in memory).
			</li>
			<li>
				The <strong>view</strong> visualizes the data in the model.
				For example, in the figure above, the view is the form itself and the controls on it, including the text fields and login button.
				The view's job is to render the controls, listen for input from the user (e.g., clicking the login button), and display any output the controller decides to provide (e.g., form validation feedback).
			</li>
			<li>
				The <strong>controller</strong> implements the logic of the user interface.
				In our login form example above, that includes validating the username and password (e.g., neither can be empty), and submitting the information when the login button is pressed.
				The controller gets and stores data when necessary, and tells the view to update itself as the model changes.
			</li>
		</ul>
		
		<p>
			Now, if every individual widget in a user interface is its own self-contained model-view-controller architecture, how are all of these individual widgets composed together into a user interface?
			There are three big ideas that stitch together individual widgets into an entire interface.
		</p>
		
		<p>
			First, all user interfaces are structured as <strong>hierarchies</strong> in which one widget can contain zero or more other "child" widgets, and each widget has a parent, except for the "root" widget (usually a window).
			For instance, here's the Facebook post UI we were discussing earlier and its corresponding hierarchy:
		</p>
		
		<img src="images/ui-hierarchy.jpg" class="img-responsive" alt="A diagram mapping the Facebook post user interface to the component hierarchy it is composed of, including a post, an avatar icon, an editor, a text box, a label, and an emoticon and photo upload widget." />

		<p>
			Notice how there are some components in the tree above that aren't visible in the UI (the "post", the "editor", the "special input" container).
			Each of these are essentially containers that group components together.
			This brings us to the second big idea of <strong>layout</strong>, in which the children of each component are organized spatially according to some layout rule.
			For example, the special input widgets are laid out in a horizontal row within the special inputs container and the special inputs container itself is laid out right aligned in the "editor" container.
			Each component has its own layout rules that govern the display of its children.
		</p>
		
		<p>
			Finally, <strong>event propagation</strong> is the process by which user interface events move from a physical device to a particular user interface component.
			Each device has its own process, because it has its own semantics. For instance:
		</p>
		
		<ul>
			<li>
				A <strong>mouse</strong> emits mouse move events and button presses and releases.
				All of these are emitted as discrete events to operating system.
				Particular sequences events are aggregated into <em>synthetic</em> events like a click (a mouse press followed by a mouse release).
				When the operating system receives them, it first decides which window will receive those events by comparing the position of the mouse to the position and layering of the windows, finding the topmost window that contains the mouse position.
				Then, the window decides which component will handle the event by finding the topmost component whose spatial boundaries contain the mouse.
				That event is then sent to that component.
				If the component doesn't handle the event (e.g., someone clicks on some text in a web browser that doesn't respond to clicks), the event may be <em>propagated</em> to its parent, and to it's parent's parent, etc, seeing if any of the ancestors in the component hierarchy want to handle the event.
				Every user interface framework handles this propagation slightly differently, but most follow this basic pattern.
			</li>
			<li>
				A <strong>keyboard</strong> emits key down and key up events, each with a <em>key code</em> that corresponds to the physical key that was pressed.
				As with a mouse, sequences are synthesized into other events (e.g., a key down followed by a key up with the same key is a key "press").
				Whereas a mouse has a position, a keyboard does not, and so operating systems maintain a notion of <em>window focus</em> to determine which window is receiving key events, and then each window maintains a notion of <em>keyboard focus</em> to determine which component receives key events.
				Operating systems are then responsible for providing a visible indicator of which component has keyboard focus (perhaps giving it a border highlight and showing a blinking text caret).
				As with mouse events, if the component with focus does not handle a keyboard event, it may be propagated to its ancestors and handled by one of them.
				For example, when you press the escape key when a confirmation dialog is in focus, the button that has focus will ignore it, but the dialog window may interpret the escape key press as a "cancel".
			</li>
			<li>
				A <strong>touch screen</strong> emits a stream of touch events, segmented by start, move, and end events.
				Other events include touch cancel events, such as when you slide your finger off of a screen to indicate you no longer want to touch.
				This low-level stream of events is converted by operating systems and applications into touch gestures.
				Most operating systems recognize a class of gestures and emit events for them as well, allowing user interface controls to respond to them.
			</li>
		</ul>
		
		<p>
			Even speech interfaces emit events. For example, digital voice assistants are continuously listening for activation commands such as "Hey Siri" or "Alexa."
			After these are detected, they begin converting speech into text, which is then matched to one or more commands.
			Applications that expose a set of commands then receive events that trigger the application to process the command.
			Therefore, the notion of input events isn't inherently tactile; it's more generally about translating low-level inputs into high-level commands.
		</p>
		
		<h2>Advances in User Interface Architecture</h2>
		
		<p>
			While the basic ideas presented above are now ubiquitous in desktop and mobile operating systems, the field of HCI has rapidly innovated beyond these original ideas.
			For instance, much of the research in the 1990's focused on building more robust, scalable, flexible, and powerful user interface toolkits for building desktop interfaces.
			The <a href="https://www.cs.cmu.edu/afs/cs/project/amulet/www/amulet-home.html">Amulet toolkit</a> was one of the most notable of these, offering a unified framework for supporting graphical objects, animation, input, output, commands, and undo (<a href="https://doi.org/10.1109/32.601073">Myers et al. 1997</a>).
			At the same time, there was extensive work on constraint systems, which would allow interface developers to declaratively express rules the interface must follow (e.g., this button should always be next to this other button) (<a href="http://dx.doi.org/10.1145/215585.215708">Bharat and Hudson 1995</a>, <a href="http://dx.doi.org/10.1145/237091.237112">Hudson and Smith 1996</a>).
			Other projects sought to make it easier to "skin" the visual appearance of interfaces without having to modify a user interface implementation (<a href="http://dx.doi.org/10.1145/263407.263539">Hudson and Smith 1997</a>).
			Research in the 2000's shifted to deepen these ideas.
			For example, some work investigated alternatives to component hierarchies such as scene graphs (<a href="http://dx.doi.org/10.1145/1029632.1029677">Huot et al. 2004</a>) and views across multiple machines (<a href="https://doi.org/10.1145/964696.964711">Lecolinet 2003</a>), making it easier to build highly animated and connected interfaces.
			Some works deepened architectures for supporting undo and redo (<a href="http://dx.doi.org/10.1145/354401.354409">Edwards et al. 2000</a>).
			Many of these ideas are now common in modern user interface toolkits, especially the web, in the form of CSS and it's support for constraints, animations, and layout separate from interface behavior.
		</p>

		<p>
			Other research has looked beyond traditional WIMP interfaces, creating new architectures to support new media.
			The DART toolkit, for example, invented several abstractions for augmented reality applications (<a href="https://doi.org/10.1145/2642918.2647369">Gandy and MacIntyre 2014</a>).
			Researchers contributed architectures for digital ink applications (<a href="http://dx.doi.org/10.1145/354401.354412">Hong and Landay 2000</a>), zoomable interfaces (<a href="http://dx.doi.org/10.1145/354401.354754">Bederson et al. 2000</a>), peripheral displays that monitor user attention (<a href="http://dx.doi.org/10.1145/1029632.1029676">Matthews et al. 2004</a>), data visualizations (<a href="https://doi.org/10.1109/TVCG.2009.174">Bostock and Heer 2009</a>), tangible user interfaces made of physical components (<a href="http://dx.doi.org/10.1145/502348.502388">Greenburg and Fitchett 2011</a>, <a href="http://dx.doi.org/10.1145/985692.985743">Klemmer et al. 2004</a>), interfaces based on proximity between people and objects (<a href="https://doi.org/10.1145/2047196.2047238">Marquardt et al. 2011</a>), and multi-touch gestures (<a href="https://doi.org/10.1145/2380116.2380176">Kin et al. 2012</a>).
			Another parallel sequence of work explored the general problem of handling events that are uncertain or continuous, investigating novel architectures and error handling strategies to manage uncertainty (<a href="https://doi.org/10.1145/142621.142650">Hudson and Newell 1992</a>, <a href="http://dx.doi.org/10.1145/354401.354407">Mankoff et al. 2000</a>, <a href="https://doi.org/10.1145/1866029.1866039">Schwarz et al. 2010</a>).
			Each one of these toolkits contributed new types of events, event handling, event propagation, synthetic event processing, and model-view-controller architectures tailored to these inputs.
		</p>

		<p>
			While much of the work in user interface architecture has sought to contribute new architectural ideas for user interface construction, some have focused on ways of <em>modifying</em> user interfaces without modifying their underlying code.
			For example, one line of work has explored how to express interfaces abstractly, so these abstract specifications can be used to generate many possible interfaces depending on which device is being used (<a href="http://dx.doi.org/10.1145/192426.192443">Edwards and Mynatt 1994</a>, <a href="https://doi.org/10.1145/571985.572008">Nichols et al. 2002</a>, <a href="https://doi.org/10.1145/1166253.1166298">Nichols et al. 2006</a>).
			Other systems have invented ways to modify interface behavior by intercepting events at runtime and forcing applications to handle them differently (<a href="https://doi.org/10.1145/2047196.2047226">Eagan et al. 2011</a>).
			Some systems have explored ways of directly manipulating interface layout during use (<a href="https://doi.org/10.1145/1166253.1166301">Stuerzlinger et al. 2006</a>) and transforming interface presentation (<a href="http://dx.doi.org/10.1145/263407.263537">Edwards et al. 1997</a>).
			More recent techniques have taken interfaces as implemented, reverse engineered their underlying commands, and generated new, more accessible, more usable, and more powerful interfaces based on these reverse engineered models (<a href="https://doi.org/10.1145/3025453.3025506">Swearngin et al. 2017</a>).
		</p>

		<p>
			A smaller but equally important body of work has investigated ways of making interfaces easier to test and debug.
			Some of these systems expose information about events, event handling, and finite state machine state (<a href="http://dx.doi.org/10.1145/263407.263542">Hudson et al. 1997</a>).
			Some have invented ways of recording and replaying interaction data with interfaces to help localize defects in user interface behavior (<a href="https://doi.org/10.1145/1866029.1866048">Newman et al. 2010</a>, <a href="http://dx.doi.org/10.1145/2501988.2502050">Burg et al. 2013</a>).
			Some have even investigated the importance of testing security vulnerabilities in user interfaces, as interfaces like copy and paste transact and manipulate sensitive information (<a href="https://doi.org/10.1145/2380116.2380147">Roesner et al. 2012</a>).
		</p>

		<p>
			Considering this body of work as a whole, there are some patterns that become clear:
			</p>
		
		<ul>
			<li>Model-view-controller is a ubiquitous architectural style in user interface implementation.</li>
			<li>User interface toolkits are essential to making it easy to implement interfaces.</li>
			<li>New input techniques require new user interface architectures, and therefore new user interface toolkits.</li>
			<li>Interfaces can be automatically generated, manipulated, inspected, and transformed, but only within the limits of the architecture in which they are implemented.</li>
			<li>The architecture an interface is built in determines what is difficult to test and debug.</li>
		</ul>

		<p>These "laws" of user interface implementation can be useful for making predictions about the future. For example, if someone proposes incorporating a new sensor in a device, subtle details in the sensor's interactive potential may require new forms of testing and debugging, new architectures, and potentially new toolkits to fully leverage its potential. That's a powerful prediction to be able to make and one that many organizations overlook when they ship new devices.</p>

		<center class="lead"><a href="accessibility.html">Next chapter: Accessibility</a></center>
	
		<h2>Further reading</h2>

		<p>Eytan Adar, Mira Dontcheva, and Gierad Laput. 2014. CommandSpace: modeling the relationships between tasks, descriptions and features. In Proceedings of the 27th annual ACM symposium on User interface software and technology (UIST '14). ACM, New York, NY, USA, 167-176.</p>

		<p>Krishna A. Bharat and Scott E. Hudson. 1995. <a href="http://dx.doi.org/10.1145/215585.215708">Supporting distributed, concurrent, one-way constraints in user interface applications</a>. In Proceedings of the 8th annual ACM symposium on User interface and software technology (UIST '95). ACM, New York, NY, USA, 121-132.</p>

		<p>Benjamin B. Bederson, Jon Meyer, and Lance Good. 2000. <a href="http://dx.doi.org/10.1145/354401.354754">Jazz: an extensible zoomable user interface graphics toolkit in Java</a>. In Proceedings of the 13th annual ACM symposium on User interface software and technology (UIST '00). ACM, New York, NY, USA, 171-180.</p>

		<p>Brian Burg, Richard Bailey, Amy J. Ko, and Michael D. Ernst. 2013. <a href="http://dx.doi.org/10.1145/2501988.2502050">Interactive record/replay for web application debugging</a>. In Proceedings of the 26th annual ACM symposium on User interface software and technology (UIST '13). ACM, New York, NY, USA, 473-484.</p>

		<p>Bostock, Michael, and Jeffrey Heer. <a href="https://doi.org/10.1109/TVCG.2009.174">Protovis: A graphical toolkit for visualization.</a> IEEE transactions on visualization and computer graphics 15, no. 6 (2009).</p>
		
		<p>James R. Eagan, Michel Beaudouin-Lafon, and Wendy E. Mackay. 2011. <a href="https://doi.org/10.1145/2047196.2047226">Cracking the cocoa nut: user interface programming at runtime</a>. In Proceedings of the 24th annual ACM symposium on User interface software and technology (UIST '11). ACM, New York, NY, USA, 225-234.</p>

		<p>W. Keith Edwards, Scott E. Hudson, Joshua Marinacci, Roy Rodenstein, Thomas Rodriguez, and Ian Smith. 1997. <a href="http://dx.doi.org/10.1145/263407.263537">Systematic output modification in a 2D user interface toolkit</a>. In Proceedings of the 10th annual ACM symposium on User interface software and technology (UIST '97). ACM, New York, NY, USA, 151-158.</p>

		<p>W. Keith Edwards and Elizabeth D. Mynatt. 1994. <a href="http://dx.doi.org/10.1145/192426.192443">An architecture for transforming graphical interfaces</a>. In Proceedings of the 7th annual ACM symposium on User interface software and technology (UIST '94). ACM, New York, NY, USA, 39-47.</p>

		<p>W. Keith Edwards, Takeo Igarashi, Anthony LaMarca, and Elizabeth D. Mynatt. 2000. <a href="http://dx.doi.org/10.1145/354401.354409">A temporal model for multi-level undo and redo</a>. In Proceedings of the 13th annual ACM symposium on User interface software and technology (UIST '00). ACM, New York, NY, USA, 31-40.</p>

		<p>Saul Greenberg and Chester Fitchett. 2001. <a href="http://dx.doi.org/10.1145/502348.502388">Phidgets: easy development of physical interfaces through physical widgets</a>. In Proceedings of the 14th annual ACM symposium on User interface software and technology (UIST '01). ACM, New York, NY, USA, 209-218.</p>

		<p>Jason I. Hong and James A. Landay. 2000. <a href="http://dx.doi.org/10.1145/354401.354412">SATIN: a toolkit for informal ink-based applications</a>. In Proceedings of the 13th annual ACM symposium on User interface software and technology (UIST '00). ACM, New York, NY, USA, 63-72.</p>

		<p>Scott E. Hudson and Gary L. Newell. 1992. <a href="https://doi.org/10.1145/142621.142650">Probabilistic state machines: dialog management for inputs with uncertainty</a>. In Proceedings of the 5th annual ACM symposium on User interface software and technology (UIST '92). ACM, New York, NY, USA, 199-208.</p>

		<p>Scott E. Hudson and Ian Smith. 1996. <a href="http://dx.doi.org/10.1145/237091.237112">Ultra-lightweight constraints</a>. In Proceedings of the 9th annual ACM symposium on User interface software and technology (UIST '96). ACM, New York, NY, USA, 147-155.</p>

		<p>Scott E. Hudson and Ian Smith. 1997. <a href="http://dx.doi.org/10.1145/263407.263539">Supporting dynamic downloadable appearances in an extensible user interface toolkit</a>. In Proceedings of the 10th annual ACM symposium on User interface software and technology (UIST '97). ACM, New York, NY, USA, 159-168.</p>

		<p>Scott E. Hudson, Roy Rodenstein, and Ian Smith. 1997. <a href="http://dx.doi.org/10.1145/263407.263542">Debugging lenses: a new class of transparent tools for user interface debugging</a>. In Proceedings of the 10th annual ACM symposium on User interface software and technology (UIST '97). ACM, New York, NY, USA, 179-187.</p>

		<p>Stéphane Huot, Cédric Dumas, Pierre Dragicevic, Jean-Daniel Fekete, and Gerard Hégron. 2004. <a href="http://dx.doi.org/10.1145/1029632.1029677">The MaggLite post-WIMP toolkit: draw it, connect it and run it</a>. In Proceedings of the 17th annual ACM symposium on User interface software and technology (UIST '04). ACM, New York, NY, USA, 257-266.</p>

		<p>Kenrick Kin, Bjoern Hartmann, Tony DeRose, and Maneesh Agrawala. 2012. <a href="https://doi.org/10.1145/2380116.2380176">Proton++: a customizable declarative multitouch framework</a>. In Proceedings of the 25th annual ACM symposium on User interface software and technology (UIST '12). ACM, New York, NY, USA, 477-486.</p>

		<p>Scott R. Klemmer, Jack Li, James Lin, and James A. Landay. 2004. <a href="http://dx.doi.org/10.1145/985692.985743">Papier-Mache: toolkit support for tangible input</a>. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '04). ACM, New York, NY, USA, 399-406.</p>

		<p>Amy J. Ko and Brad A. Myers. 2005. <a href="http://dx.doi.org/10.1145/1095034.1095037">Citrus: a language and toolkit for simplifying the creation of structured editors for code and data</a>. In Proceedings of the 18th annual ACM symposium on User interface software and technology (UIST '05). ACM, New York, NY, USA, 3-12.</p>

		<p>Eric Lecolinet. 2003. <a href="https://doi.org/10.1145/964696.964711">A molecular architecture for creating advanced GUIs</a>. In Proceedings of the 16th annual ACM symposium on User interface software and technology (UIST '03). ACM, New York, NY, USA, 135-144.</p>

		<p>Jennifer Mankoff, Scott E. Hudson, and Gregory D. Abowd. 2000. <a href="http://dx.doi.org/10.1145/354401.354407">Interaction techniques for ambiguity resolution in recognition-based interfaces</a>. In Proceedings of the 13th annual ACM symposium on User interface software and technology (UIST '00). ACM, New York, NY, USA, 11-20.</p>

		<p>Nicolai Marquardt, Robert Diaz-Marino, Sebastian Boring, and Saul Greenberg. 2011. <a href="https://doi.org/10.1145/2047196.2047238">The proximity toolkit: prototyping proxemic interactions in ubiquitous computing ecologies</a>. In Proceedings of the 24th annual ACM symposium on User interface software and technology (UIST '11). ACM, New York, NY, USA, 315-326.</p>

		<p>Mark W. Newman, Mark S. Ackerman, Jungwoo Kim, Atul Prakash, Zhenan Hong, Jacob Mandel, and Tao Dong. 2010. <a href="https://doi.org/10.1145/1866029.1866048">Bringing the field into the lab: supporting capture and replay of contextual data for the design of context-aware applications</a>. In Proceedings of the 23nd annual ACM symposium on User interface software and technology (UIST '10). ACM, New York, NY, USA, 105-108.</p>

		<p>Tara Matthews, Anind K. Dey, Jennifer Mankoff, Scott Carter, and Tye Rattenbury. 2004. <a href="http://dx.doi.org/10.1145/1029632.1029676">A toolkit for managing user attention in peripheral displays</a>. In Proceedings of the 17th annual ACM symposium on User interface software and technology (UIST '04). ACM, New York, NY, USA, 247-256.</p>

		<p>Myers, Brad A., Richard G. McDaniel, Robert C. Miller, Alan S. Ferrency, Andrew Faulring, Bruce D. Kyle, Andrew Mickish, Alex Klimovitski, and Patrick Doane. <a href="https://doi.org/10.1109/32.601073">The Amulet environment: New models for effective user interface software development</a>. IEEE Transactions on software engineering 23, no. 6 (1997): 347-365.</p>

		<p>Jeffrey Nichols, Brandon Rothrock, Duen Horng Chau, and Brad A. Myers. 2006. <a href="https://doi.org/10.1145/1166253.1166298">Huddle: automatically generating interfaces for systems of multiple connected appliances</a>. In Proceedings of the 19th annual ACM symposium on User interface software and technology (UIST '06). ACM, New York, NY, USA, 279-288.</p>

		<p>Jeffrey Nichols, Brad A. Myers, Michael Higgins, Joseph Hughes, Thomas K. Harris, Roni Rosenfeld, and Mathilde Pignol. 2002. <a href="https://doi.org/10.1145/571985.572008">Generating remote control interfaces for complex appliances</a>. In Proceedings of the 15th annual ACM symposium on User interface software and technology (UIST '02). ACM, New York, NY, USA, 161-170.</p>

		<p>Stephen Oney, Brad Myers, and Joel Brandt. 2012. <a href="https://doi.org/10.1145/2380116.2380146">ConstraintJS: programming interactive behaviors for the web by integrating constraints and states</a>. In Proceedings of the 25th annual ACM symposium on User interface software and technology (UIST '12). ACM, New York, NY, USA, 229-238.</p>

		<p>Stephen Oney, Brad Myers, and Joel Brandt. 2014. <a href="https://doi.org/10.1145/2642918.2647358">InterState: a language and environment for expressing interface behavior</a>. In Proceedings of the 27th annual ACM symposium on User interface software and technology (UIST '14). ACM, New York, NY, USA, 263-272.</p>

		<p>Hubert Pham, Justin Mazzola Paluska, Rob Miller, and Steve Ward. 2012. <a href="https://doi.org/10.1145/2380116.2380141">Clui: a platform for handles to rich objects</a>. In Proceedings of the 25th annual ACM symposium on User interface software and technology (UIST '12). ACM, New York, NY, USA, 177-188.</p>

		<p>Franziska Roesner, James Fogarty, and Tadayoshi Kohno. 2012. <a href="https://doi.org/10.1145/2380116.2380147">User interface toolkit mechanisms for securing interface elements</a>. In Proceedings of the 25th annual ACM symposium on User interface software and technology (UIST '12). ACM, New York, NY, USA, 239-250.</p>

		<p>Julia Schwarz, Scott Hudson, Jennifer Mankoff, and Andrew D. Wilson. 2010. <a href="https://doi.org/10.1145/1866029.1866039">A framework for robust and flexible handling of inputs with uncertainty</a>. In Proceedings of the 23nd annual ACM symposium on User interface software and technology (UIST '10). ACM, New York, NY, USA, 47-56.</p>

		<p>Wolfgang Stuerzlinger, Olivier Chapuis, Dusty Phillips, and Nicolas Roussel. 2006. <a href="https://doi.org/10.1145/1166253.1166301">User interface facades: towards fully adaptable user interfaces</a>. In Proceedings of the 19th annual ACM symposium on User interface software and technology (UIST '06). ACM, New York, NY, USA, 309-318.</p>

		<p>Amanda Swearngin, Amy J. Ko, and James Fogarty. 2017. <a href="https://doi.org/10.1145/3025453.3025506">Genie: Input Retargeting on the Web through Command Reverse Engineering</a>. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI '17). ACM, New York, NY, USA, 4703-4714.</p>

		<script type="text/javascript">
		
			var _gaq = _gaq || [];
			_gaq.push(['_setAccount', 'UA-10917999-1']);
			_gaq.push(['_trackPageview']);
			
			(function() {
			var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
			ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
			var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
			})();
		
		</script>

	</body>
	
</html>

