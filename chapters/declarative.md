If you don't know the history of computing, it's easy to overlook the fact that all of the interfaces that people used to control computers before the graphical user interface were _programming_ interfaces. The distinction between programming interfaces and interactive interfaces comes down to three critical attributes of programming<blackwell02>: 

* *No direct manipulation*. In direct manipulation, concrete actions on data result in concrete, immediate feedback about the effect of those actions. Dragging a file from one folder to another immediately moves the file, both visually, and on disk. In programming, nothing is immediate: one _declares_ what action they want to occur, and then at some later point in the future when the program is executed, that effect occurs.
* *Use of notation*. In order to declare what future behavior a computer should do when programming, one must use notations to represent those future actions. In the same way that English is a notation for expressing action, "can you bring me that book?", programming languages use notation to symbolically refer to computer action.
*Use of abstraction*. An inherent side effect of using notations is that one must also use _abstractions_ to refer to a computer's actions. For example, when one uses the Unix command line program `mv file.txt ../` (which moves file.txt to it's parent directory), they are using a command, a file name, and a symbolic reference `../`  to refer to the parent directory of the current directory. Commands, names, and relative path references are all _abstractions_ in that they remove detail, allowing a user to interact with the _ideas_ of files and file movement. Visual icons that represent files are also abstractions, but they support direct manipulation, such as moving files from one folder to another.

Because of the indirection, notation, and abstraction inherent to programming interfaces, I will refer to them as "declarative" interfaces from here on. In these interfaces, a user declares to the computer what it wants, and then the computer compiles and executes the instructions (rather than acting directly on data).
 
Why are we talking about programming in a book about user interfaces? Well, declarative interfaces _are_ user interfaces. They're just user interfaces that have very different properties than the interactive user interfaces we're all familiar with. Clearly, all of the properties above come with great cost: no immediate feedback, having to learn a programming language, and having to think abstractly are all much more difficult than having to learn a graphical user interface. But with all of that additional learning also comes great power. People can use programming languages to get computers to do entirely new things, to control their behavior more precisely, and more importantly, to automate tasks that humans can't do or can't do nearly as quickly.
 
|https://www.youtube.com/embed/OVTu4XcmnwE|How does a compiler, interpreter, and CPU work?|An explanation of programming language translators|All Aboot Software|
 
What are declarative interfaces? Ultimately, they're software that _translate_ some expression of a program (a text file with code in it, or a hierarchical tree of language constructs in [block-based editor|https://developers.google.com/blockly/] into a representation a computer can execute, usually some series of instructions that consist of low level operations such as arithmetic, reading and writing data values in memory, and jumping to other instructions. In textual languages, there are lexers, which decompose code into _tokens_, which are then parsed according to a formal grammar into an _abstract syntax tree_. In the case of block-based editors, users use an editor to directly manipulate an abstract syntax tree. Abstract syntax trees are them _compiled_ into instructions, which the program can then later execute. Sometimes the instructions are  "interpreted," in that they are executed by another program, and some are directly executed by a physical CPU. This translation process is what makes programming languages so hard to learn: it creates great distance between notational expressions of what a user wants and the eventual behavior of the compiled program and its output.
 

# Why are declarative interfaces hard to learn?

However, this distance, conceptually, is no different from any other user interface. Because declarative interfaces are user interfaces, they also have [gulfs of evaluation and execution, and affordances and feedback that bridge them|theory]. Their gulfs are just much larger because of translation. Because textual programs are parsed according to a grammar (like in natural languages, but more strict), there are numerous ways to violate the grammar, requiring a user to know the rules of the grammar to write correct programs. Because the functional affordances in a programming language can be combined in infinite ways, there are an _infinite_ number of programs that can exist. How can one figure out which program will achieve a user's goal? Code examples, documentation, Stack Overflow, design patterns, software architecture skills, and other media are key to bridging this gulf. Similarly, because programs can execute hundreds, thousands, even billions of instructions per second, and even simple programs can execute in a multitude of different ways depending on the input they receive, there's a really big space of possible program outputs that a user might need to understand. That's a big gulf of evaluation, and programs mostly provide no feedback about _how_ they execute. Testing, debugging, and monitoring tools help bridge this gulf by providing data and explanations about how a program executed in response to input.
 

Early in my research career, I set out to document some of the major categories of gulfs of execution and evaluation in declarative interfaces. I found six<ko04>:
 

* *Design barriers*. Declarative interfaces each have their own distinct things that can be easily expressed. Deciding _what_ to express therefore depends on knowledge of what _can_ be expressed. People have to bridge this gulf of execution by learning the expressive possibilities before they can decide what they want to express.
* *Selection barriers*. Once someone knows what they want to express, finding the abstractions that are most helpful in expressing it is another challenge. Which parts of a programming language and API are most useful in expressing some computation? People have to bridge this gulf of execution before writing any code.
* *Use barriers*. Once someone finds some abstractions that are useful for expressing what they want, they have to learn how to _use_ these abstractions. For example, when using a function in an API, they might have to learn what kinds of arguments to send to the function, and what that function might return. People have to bridge this gulf of execution to know what code to write.
* *Coordination barriers*. Once someone finds what abstractions are relevant, they have to learn how to compose them together to do what they want. This is because different abstractions in a declaractive interface can interact with each other, requiring knowledge of these interactions. People have to bridge this gulf of execution to know what code to write.
* *Understanding barriers*. Once someone has written some code, it's probably not exactly right, which means they will have to generate explanations of what might be wrong with their program. Generating these possibilities requires learning what _can_ go wrong in a program. People have to bridge this gulf of evaluation before they can fix defects in their code.
* *Information barriers*. Once someone has an idea of what _might_ be wrong in their program, they have to gather information about how the program executed to check their guess. Getting this information requires learning how to use things like testing and debugging tools, which pose their own set of gulfs.

These various barriers reveal why learning to code is so challenging and requires so much learning relative to graphical user interfaces.
 
Another way to understand the complexities of declarative interfaces is to understand how they vary. The Cognitive Dimensions of Notations framework<green96> described several dimensions along with notations, and particularly programming notations, vary. Here are a few notable examples:
 
* *Error-proneness*.  Some notations are more error prone than others.  For example, in the JavaScript programming language, the symbol "+", when preceded by a variable storing a text string, automatically converts the value after the plus to a string.  That means that programmers can accidentally end up with text values when they intended to have a numerical value. The Java programming language, in contrast, identifies these type errors before a program runs, making them impossible.
* *Hidden dependencies*. Some notations hide the underlying structure of a program more than others. For example, in most modern imperative programming languages, the sequence in which a function in a program will execute is pretty clear, since they execute in order. But things get less clear once functions start invoking other functions: this creates a network of function calls that cannot be easily seen from a program's text. 
* *Premature commitment* Some notations require a user to make several decisions before they have the information they need, others provide flexibility. For example, Excel is quite good at letting users type in a bunch of data, later add formulas on top of that data as needed, regardless of how the data is organized in a spreadsheet. In contrast, doing the same calculations in Python requires data to be organized and formatted before it can be read in, long before a user is allowed to explore the data with code. 

The learning barriers above interact with these dimensions of notations. Languages that avoid premature commitment might be more error-prone, and may require more learning, for example. Languages that make some dependencies hidden might reveal others, shifting what someone has to learn.
 
# How can declarative interfaces be made easier to use?
 While there is a vast literature on programming languages and tools, and much of it focuses on bridging gulfs of execution and evaluation, in this chapter, we'll focus on the contributions that HCI researchers have made to solving these problems, as they demonstrate the rich, under-explored design space of ways that people can program computers beyond using general purpose languages. Much of this work can be described as supporting _end-user programming_, which is any programming that someone does as a means to accomplishing some other goal<ko11>. For example, a teacher writing formulas in a spreadsheet to compute grades, a child using [Scratch|https://scratch.mit.edu/] to create an animation, or a data scientist writing a Python script to wrangle some data&mdash;none of these people are writing code for the code itself (as professional software engineers do), they're writing code for the output their program will produce (the teacher wants the grades, the child wants the animation, the data scientist wants the wrangled data).
 
|sikuli.png|A screenshot of the Sikuli system, showing a loop that waits for a particular image to appear on a map before showing a popup that says the bus has arrived|Sikuli<yeh09>|Tom Yeh|

This vast range of domains in which declarative interfaces can be applied has lead to an abundance of unique declarative interfaces. For example, several researchers have explored ways to automate interaction with user interfaces, to take repetitive tasks and automate them. One such system is Sikuli (above), which allows users to use screenshots of user interfaces to write scripts that automate interactions<yeh09>. Similar systems have used similar ideas to allow users to write simple programs to automate web tasks. CoScripter<leshed08> allowed a user to demonstrate an interaction with an interface, which generated a program in a natural-language-like syntax that could then be executed to replay that action. CoCo<lau10> allowed a user to write a command like "get road conditions for highway 88," which the system then translates into operations on a website using the user's previous browsing history and previously recorded web scripts. Two related contributions used the metaphor of a command line interface for the web and desktop applications, one taking natural language descriptions of task and recommending commands<miller08> and the other using a "sloppy" syntax of keywords to recommend commands within an application<little06>. All of these ideas bridge the gulf of execution, helping a user express their goal in terms they understand such as demonstrating an action, selecting part of an interface, or describing their goal, then having the system translate these into an executable program.

|vega.png|A screenshot of a Vega program, which declares a scatterplot with brushing and linking feature. |Vega<satyanarayan14>|Satyanarayan et al.|

Another major focus has been supporting people interacting with data. Some systems like Vega above have offered new programming languages for declaratively specifying data visualizations<satyanarayan14>. Others have provided programmatic means for wrangling and transforming data with interactive support for previewing the effects of candidate transformation programs<guo11,mayer15>. One system looked at interactive ways of helping users search and filter data with regular expressions by identifying examples of outlier data<miller01>, whereas other systems have helped translate examples of desired database queries into SQL queries<abouzied12>. Again, a major goal of all of these systems is to help bridge the gulf of evaluation between a user's goal and the program necessary to achieve it.
 
|mavo.png|A screenshot of the Mavo system, showing a simple HTML body with attribute annotations and the corresponding to do list application the annotations specify|Mavo<verou16>|Verou et al.|
 
Some systems have attempted to support more ambitious automation, empowering users to create entire applications that better support their personal information needs. For example, many systems have combined spreadsheets with other simple scripting languages to enable users to write simple web applications with rich interfaces, using the spreadsheet as a database<chang14,benson14>. Other systems like Mavo above and Dido have encapsulated the entire application writing process to just editing HTML by treating HTML as a specification for both the layout of a page and the layout of data<verou16,karger09>. An increasing number of systems have explored home automation domains, finding clear tradeoffs between simplicity and expressiveness in rule-based programs<brich17>.
 
|hands.jpg|A screenshot of the Hands system, showing a program that animates bees, and a card with bee properties.|Hands<pane02>|Pane and Myers|

Perhaps the richest category of end-user programming systems are those supporting the creation of games. Hundreds of systems have provided custom programming languages and development environments for authoring games, ranging from simple game mechanics to entire general purpose programming languages to support games<kelleher05>. For example, the system above, called Hands, carefully studied how children express computation with natural language, and designed a programming language inspired by the computational ideas inherent in children's reasoning about game behavior<pane02>. Other systems, most notably Gamut<mcdaniel97>, used a technique called _programming by demonstration_, in which users demonstrate the behavior they want the computer to perform, and the computer generalizes that into a program that can be executed later on a broader range of situations than the original demonstration. Gamut was notable for its ability to support the construction of an _entire_ application by demonstration, unlike many of the systems discussed above, which only partially used demonstration.

|https://www.youtube.com/embed/fP8swbzeDuY|The Whyline for Alice|Whyline<ko04>|Amy J. Ko| 

The vast majority of innovations for declarative interfaces have focused on bridging the gulf of execution. Fewer systems have focused on bridging gulfs of evaluation by supporting, testing, and debugging behaviors a user is trying to understand. One from my own research was a system called the Whyline (see the video above), which allowed users to ask "why" and "why not" questions when their program did something they didn't expect, bridging a gulf of evaluation. It identified questions by scanning the user's program for all possible program outputs, and answered questions by precisely trying the cause of every operation in the program, reasoning backwards about the chain of causality that caused an unwanted behavior or prevented a desired behavior. More recent systems have provided similar debugging and program comprehension support for understanding web pages<burg15,hibschman15>, machine learned classifiers<patel10,kulesza09>, and even embedded systems that use a combination of hardware and software to define a user interface<strasnick17,mcgrath17>.
 
---

One way to think about all of these innovations is as trying to bring all of the benefits of graphical user interfaces&mdash;immediate feedback, direct manipulation, and concreteness&mdash;to notations that inherently don't have those properties, by augmenting programming _environments_ with these features. This progress is blurring the distinction between declarative interfaces and interactive interfaces, bringing the power of programming to broader and more diverse audiences.
